{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNkk3UicQpH7UYI6MJtRGVs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ethane66/MSFT-Talent-AI/blob/main/Session_19.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "realiza este ejercicio en google colab:\n",
        " Ejercicio práctico: Generador de Texto Creativo con GPT-2\n",
        "Objetivo:\n",
        " Aplicar los conocimientos sobre modelos Transformers y la librería Hugging Face para crear un generador de texto que produzca contenido coherente a partir de un tema dado.\n",
        "\n",
        "Instrucciones:\n",
        "Abre un notebook nuevo en Google Colab.\n",
        "\n",
        "\n",
        "Instala la librería transformers si no lo has hecho aún:\n",
        "\n",
        "!pip install transformers\n",
        "Carga el modelo GPT-2 y su tokenizador desde Hugging Face.\n",
        "\n",
        "\n",
        "Escribe un prompt inicial (puedes elegir uno propio o usar uno de los ejemplos abajo).\n",
        "\n",
        "\n",
        "Genera una continuación del texto usando el modelo.\n",
        "\n",
        "\n",
        "Modifica los parámetros max_length, do_sample, top_k y top_p para experimentar con diferentes resultados.\n",
        "\n",
        "\n",
        "Añade comentarios al notebook explicando:\n",
        "\n",
        "\n",
        "Por qué elegiste ese prompt.\n",
        "\n",
        "Lo eleji porque me parecia curioso que se podia inventar sobre un mundo con la IA en un futuro que se piensa que va ser\n",
        "\n",
        "Qué observaste al cambiar los parámetros.\n",
        "\n",
        "Si le pedia pocas letras como 100 no era entendible lo que decia pero era en español pero le cambie el limite a 350 y se seguia sin tener sentido lo que decia y ademas cambio al ingles\n",
        "\n",
        "Qué aplicaciones reales podrías imaginar para este modelo.\n",
        "\n",
        "Creacion de contenido como historia ficticias"
      ],
      "metadata": {
        "id": "LuxbKM_KEMkJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PeFT30_D3S7",
        "outputId": "bb0790fe-dbbf-4e3e-b7a2-6ba0572d968a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.49.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Había una vez un mundo donde la inteligencia artificialidad de la libre de la libre de la libre, a una de la fonctione una del consejo para donde cada una comer a la fez que los raritos. In so far as it comes to me, and I am able to provide the information I am able to, this has been done, it is all of my control and is not on the books. As soon as I have finished all this I will come back and start searching for someone to talk to about the matter, and to make sure it is done right.\n",
            "\n",
            "\n",
            "And of course if it is found out, it would be a great help. I also had to go to the person I called when I got to the hotel, and to check on the situation of the staff. I have never met the person so kindly that it has made it very difficult. I will just try to find something of his character as well, that I can do for him. He is one that I always look out for and he can be of any kind of help if needed, but he is very good at being around people. In fact, I have had the pleasure of seeing him the last couple of months. We have been looking for the best places for people to see each other to take a nap, and we are seeing some very lucky friends and families here.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Instalamos la librería transformers si no está instalada\n",
        "!pip install transformers\n",
        "\n",
        "# Importamos las librerías necesarias\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# Cargamos el modelo y su tokenizador\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Definimos el prompt inicial\n",
        "prompt = \"Había una vez un mundo donde la inteligencia artificial\"\n",
        "\n",
        "# Tokenizamos el prompt\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Generamos texto con parámetros ajustables\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    max_length=300,  # Longitud máxima del texto generado\n",
        "    do_sample=True,  # Permite la aleatorización en la generación\n",
        "    top_k=50,  # Considera solo las 50 palabras más probables en cada paso\n",
        "    top_p=0.9  # Nucleus sampling: selecciona palabras dentro del 90% de probabilidad acumulada\n",
        ")\n",
        "\n",
        "# Decodificamos y mostramos el texto generado\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mismo ejercicio pero en ingles"
      ],
      "metadata": {
        "id": "yJKVkUVuFaW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalamos la librería transformers si no está instalada\n",
        "!pip install transformers\n",
        "\n",
        "# Importamos las librerías necesarias\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# Cargamos el modelo y su tokenizador\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Definimos el prompt inicial\n",
        "prompt = \"There once was a world where a super advances AI existed\"\n",
        "\n",
        "# Tokenizamos el prompt\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Generamos texto con parámetros ajustables\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    max_length=300,  # Longitud máxima del texto generado\n",
        "    do_sample=True,  # Permite la aleatorización en la generación\n",
        "    top_k=50,  # Considera solo las 50 palabras más probables en cada paso\n",
        "    top_p=0.9  # Nucleus sampling: selecciona palabras dentro del 90% de probabilidad acumulada\n",
        ")\n",
        "\n",
        "# Decodificamos y mostramos el texto generado\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3QVeUSQFaLJ",
        "outputId": "615cf3d9-bce2-4131-9805-7ebb97ab3efd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.49.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There once was a world where a super advances AI existed that was a very simple task. But now with AI, where a super advances human-like capabilities that are a bit harder to explain is, it can't be done in the traditional way.\n",
            "\n",
            "And the reality is that there is always a risk. And with AI, with the new technology that the world is using, it's very possible that something even greater will be added, to make things easier for everybody to do.\n",
            "\n",
            "So as a society, we need to have that possibility. There will be, you know, some more companies that are coming after us and we will see how this comes to pass. But I think we've all agreed we need a world where a system can become super. And the danger is, we're at a point where things that we would have considered a 'super' are just not there in today's world.\n",
            "\n",
            "TONY JONES: Mr President, thank you for being with us. And if you're looking forward, can you explain the significance of the nuclear conference?\n",
            "\n",
            "CHRISTINE PULSE: Absolutely, yes. The fact is, the whole reason that we're talking about it today is because there was an explosion of people who had a nuclear test on their house. And in fact, the fact is, a lot of these people, many of them in nuclear countries, were just so desperate, they had to be out, even just go into the\n"
          ]
        }
      ]
    }
  ]
}